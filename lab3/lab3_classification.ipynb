{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU1TknRxLmIf"
   },
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l4t-ncEeLpBY"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import classification_report, roc_curve, r2_score, mean_squared_error"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HYHYnZHLZtv"
   },
   "source": [
    "# Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qp8-pF-_LdQR"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "from multiprocessing import Pool\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return -np.dot(p, np.log2(p))\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return 1 - np.dot(p, p)\n",
    "\n",
    "\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def mad_median(y):\n",
    "    return np.mean(np.abs(y - np.median(y)))\n",
    "\n",
    "\n",
    "def regression_leaf(y):\n",
    "    return np.mean(y)\n",
    "\n",
    "\n",
    "def classification_leaf(y):\n",
    "    return mode(y)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature_idx=0, threshold=0, labels=None, left=None, right=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.labels = labels\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTree(BaseEstimator):\n",
    "    def __init__(self, max_depth=100, min_samples_split=2, min_samples_leaf=1, criterion=\"entropy\",\n",
    "                 leaf_func=\"classification_leaf\"):\n",
    "        params = {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"min_samples_leaf\": min_samples_leaf,\n",
    "            \"criterion\": criterion,\n",
    "            \"leaf_func\": leaf_func\n",
    "        }\n",
    "\n",
    "        criteria_dict = {\n",
    "            \"variance\": variance,\n",
    "            \"mad_median\": mad_median,\n",
    "            \"gini\": gini,\n",
    "            \"entropy\": entropy\n",
    "        }\n",
    "\n",
    "        leaf_dict = {\n",
    "            \"regression_leaf\": regression_leaf,\n",
    "            \"classification_leaf\": classification_leaf\n",
    "        }\n",
    "\n",
    "        for param_name, param_value in params.items():\n",
    "            setattr(self, param_name, param_value)\n",
    "\n",
    "        super(DecisionTree, self).set_params(**params)\n",
    "        self._criterion_function = criteria_dict[criterion]\n",
    "        self._leaf_value = leaf_dict[leaf_func]\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "        self.current_depth = 0\n",
    "\n",
    "    def _functional(self, x_train: pd.DataFrame, y: pd.Series, feature_idx: int, threshold):\n",
    "        mask = x_train.iloc[:, feature_idx] < threshold\n",
    "        n_obj = x_train.shape[0]\n",
    "        n_left = np.sum(mask)\n",
    "        n_right = n_obj - n_left\n",
    "        if n_left > 0 and n_right > 0:\n",
    "            return (\n",
    "                    self._criterion_function(y)\n",
    "                    - (n_left / n_obj) * self._criterion_function(y.loc[mask])\n",
    "                    - (n_right / n_obj) * self._criterion_function(y.loc[~mask])\n",
    "            )\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _build_tree(self, x_train: pd.DataFrame, y: pd.Series, depth=1):\n",
    "        \"\"\"Train decision tree\"\"\"\n",
    "        max_functional = 0\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "        n_samples, n_features = x_train.shape\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return Node(labels=y)\n",
    "\n",
    "        best_mask = None\n",
    "        if depth < self.max_depth and n_samples >= self.min_samples_split and n_samples >= self.min_samples_leaf:\n",
    "            for feature_idx in range(n_features):\n",
    "                max_value = np.max(x_train.iloc[:, feature_idx])\n",
    "                min_value = np.min(x_train.iloc[:, feature_idx])\n",
    "                threshold_values = np.linspace(min_value, max_value, 5)\n",
    "                functional_values = [\n",
    "                    self._functional(x_train, y, feature_idx, threshold) for threshold in threshold_values\n",
    "                ]\n",
    "\n",
    "                best_threshold_idx = np.nanargmax(functional_values)\n",
    "\n",
    "                if functional_values[best_threshold_idx] > max_functional:\n",
    "                    max_functional = functional_values[best_threshold_idx]\n",
    "                    best_threshold = threshold_values[best_threshold_idx]\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_mask = x_train.iloc[:, feature_idx] < best_threshold\n",
    "\n",
    "        if best_feature_idx is not None and best_mask is not None:\n",
    "            return Node(\n",
    "                feature_idx=best_feature_idx,\n",
    "                threshold=best_threshold,\n",
    "                left=self._build_tree(x_train.loc[best_mask], y.loc[best_mask], depth + 1),\n",
    "                right=self._build_tree(x_train.loc[~best_mask, :], y.loc[~best_mask], depth + 1),\n",
    "            )\n",
    "        else:\n",
    "            self.current_depth = depth\n",
    "            return Node(labels=y)\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"Run training decision tree\"\"\"\n",
    "        self.root = self._build_tree(x_train, y)\n",
    "        self.max_depth = self.current_depth\n",
    "        return self\n",
    "\n",
    "    def _predict_object(self, x: pd.Series):\n",
    "        \"\"\"Prediction for one test object\"\"\"\n",
    "        node = self.root\n",
    "        while node.labels is None:\n",
    "            if x[node.feature_idx] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return self._leaf_value(node.labels)\n",
    "\n",
    "    def predict(self, x_test: pd.DataFrame) -> np.array:\n",
    "        \"\"\"Prediction for all test objects\"\"\"\n",
    "        results = np.array([self._predict_object(x_test.iloc[i]) for i in range(0, x_test.shape[0])])\n",
    "        return np.array(results)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1QrTds1LdjQ"
   },
   "source": [
    "# Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "82NoYb21LhUu"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return -np.dot(p, np.log2(p))\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return 1 - np.dot(p, p)\n",
    "\n",
    "\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def mad_median(y):\n",
    "    return np.mean(np.abs(y - np.median(y)))\n",
    "\n",
    "\n",
    "def regression_leaf(y):\n",
    "    return np.mean(y)\n",
    "\n",
    "\n",
    "def classification_leaf(y):\n",
    "    return mode(y)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature_idx=0, threshold=0, labels=None, left=None, right=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.labels = labels\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class RandomForest(BaseEstimator):\n",
    "    def __init__(self, max_depth=np.inf, min_samples_split=2, min_samples_leaf=1, criterion=\"gini\",\n",
    "                 leaf_func=\"classification_leaf\", N=10):\n",
    "        params = {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"min_samples_leaf\": min_samples_leaf,\n",
    "            \"criterion\": criterion,\n",
    "            \"leaf_func\": leaf_func,\n",
    "            \"N\": N\n",
    "        }\n",
    "\n",
    "        criterion_dict = {\n",
    "            \"variance\": variance,\n",
    "            \"mad_median\": mad_median,\n",
    "            \"gini\": gini,\n",
    "            \"entropy\": entropy\n",
    "        }\n",
    "\n",
    "        leaf_dict = {\n",
    "            \"regression_leaf\": regression_leaf,\n",
    "            \"classification_leaf\": classification_leaf\n",
    "        }\n",
    "\n",
    "        for param_name, param_value in params.items():\n",
    "            setattr(self, param_name, param_value)\n",
    "\n",
    "        super(RandomForest, self).set_params(**params)\n",
    "        self._criterion_function = criterion_dict[criterion]\n",
    "        self._leaf_value = leaf_dict[leaf_func]\n",
    "        self.bootstrap = []\n",
    "        self.forest = []\n",
    "\n",
    "    def _get_bootstrap(self, data: pd.DataFrame, target: pd.Series) -> None:\n",
    "        n_samples = data.shape[0]\n",
    "        for i in range(self.N):\n",
    "            sample_index = np.random.randint(0, n_samples, size=n_samples)\n",
    "            data_bootstrap = data.iloc[sample_index]\n",
    "            target_bootstrap = target.iloc[sample_index]\n",
    "            self.bootstrap.append((data_bootstrap, target_bootstrap))\n",
    "\n",
    "    def _get_subsample(self, len_sample: int) -> np.ndarray:\n",
    "        sample_indexes = list(range(len_sample))\n",
    "        if self._leaf_value == \"classification_leaf\":\n",
    "            len_subsample = int(np.sqrt(len_sample))\n",
    "        else:\n",
    "            len_subsample = int(np.divide(len_sample, 3))\n",
    "        subsample = np.random.choice(a=sample_indexes, size=len_subsample, replace=False)\n",
    "        return subsample\n",
    "\n",
    "    def _functional(self, x_train: pd.DataFrame, y: pd.Series, feature_idx: int, threshold):\n",
    "        mask = x_train.iloc[:, feature_idx] < threshold\n",
    "        n_obj = x_train.shape[0]\n",
    "        n_left = np.sum(mask)\n",
    "        n_right = n_obj - n_left\n",
    "        if n_left > 0 and n_right > 0:\n",
    "            return (\n",
    "                    self._criterion_function(y)\n",
    "                    - (n_left / n_obj) * self._criterion_function(y.loc[mask])\n",
    "                    - (n_right / n_obj) * self._criterion_function(y.loc[~mask])\n",
    "            )\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _build_tree(self, x_train: pd.DataFrame, y: pd.Series, depth=1):\n",
    "        \"\"\"Train decision tree\"\"\"\n",
    "        max_functional = 0\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "        n_samples, n_features = x_train.shape\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return Node(labels=y)\n",
    "\n",
    "        best_mask = None\n",
    "        if depth < self.max_depth and n_samples >= self.min_samples_split and n_samples >= self.min_samples_leaf:\n",
    "            for feature_idx in range(n_features):\n",
    "                max_value = np.max(x_train.iloc[:, feature_idx])\n",
    "                min_value = np.min(x_train.iloc[:, feature_idx])\n",
    "                threshold_values = np.linspace(min_value, max_value, 5)\n",
    "                functional_values = [\n",
    "                    self._functional(x_train, y, feature_idx, threshold) for threshold in threshold_values\n",
    "                ]\n",
    "\n",
    "                best_threshold_idx = np.nanargmax(functional_values)\n",
    "\n",
    "                if functional_values[best_threshold_idx] > max_functional:\n",
    "                    max_functional = functional_values[best_threshold_idx]\n",
    "                    best_threshold = threshold_values[best_threshold_idx]\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_mask = x_train.iloc[:, feature_idx] < best_threshold\n",
    "\n",
    "        if best_feature_idx is not None and best_mask is not None:\n",
    "            return Node(\n",
    "                feature_idx=best_feature_idx,\n",
    "                threshold=best_threshold,\n",
    "                left=self._build_tree(x_train.loc[best_mask], y.loc[best_mask], depth + 1),\n",
    "                right=self._build_tree(x_train.loc[~best_mask, :], y.loc[~best_mask], depth + 1),\n",
    "            )\n",
    "        else:\n",
    "            return Node(labels=y)\n",
    "\n",
    "    def _fit_one_tree(self, x_train: pd.DataFrame, y: pd.Series) -> Node:\n",
    "        \"\"\"Обучаем одно дерево\"\"\"\n",
    "        root = self._build_tree(x_train, y)\n",
    "        return root\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"Обучаем случайный лес\"\"\"\n",
    "        self._get_bootstrap(x_train, y)\n",
    "        for sample_obj, sample_y in self.bootstrap:\n",
    "            root = self._fit_one_tree(sample_obj, sample_y)\n",
    "            self.forest.append(root)\n",
    "\n",
    "    def _predict_object(self, x: pd.Series, node: Node):\n",
    "        \"\"\"Prediction for one test object\"\"\"\n",
    "        while node.labels is None:\n",
    "            if x[node.feature_idx] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return self._leaf_value(node.labels)\n",
    "\n",
    "    def predict(self, x_test: pd.DataFrame):\n",
    "        \"\"\"Prediction for all test objects\"\"\"\n",
    "        results = []\n",
    "        for i in range(0, x_test.shape[0]):\n",
    "            predictions_for_one = [self._predict_object(x_test.iloc[i], root) for root in self.forest]\n",
    "            prediction = self._leaf_value(predictions_for_one)\n",
    "            results.append(prediction)\n",
    "\n",
    "        return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axrszEPELhj1"
   },
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F6_32wRVLlNH"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def entropy(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return -np.dot(p, np.log2(p))\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return 1 - np.dot(p, p)\n",
    "\n",
    "\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def mad_median(y):\n",
    "    return np.mean(np.abs(y - np.median(y)))\n",
    "\n",
    "\n",
    "def regression_leaf(y):\n",
    "    return np.mean(y)\n",
    "\n",
    "\n",
    "def classification_leaf(y):\n",
    "    return mode(y)\n",
    "\n",
    "class GradientBoosting(BaseEstimator):\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.01, max_depth=3, min_samples_split=5, criterion=\"entropy\",\n",
    "                 leaf_func=\"classification_leaf\", random_state=17, loss_name=\"mse\"):\n",
    "\n",
    "        criteria_dict = {\n",
    "            \"variance\": variance,\n",
    "            \"mad_median\": mad_median,\n",
    "            \"gini\": gini,\n",
    "            \"entropy\": entropy\n",
    "        }\n",
    "\n",
    "        leaf_dict = {\n",
    "            \"regression_leaf\": regression_leaf,\n",
    "            \"classification_leaf\": classification_leaf\n",
    "        }\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.leaf_func = leaf_func\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_name = loss_name\n",
    "        self.initialization = lambda y: np.mean(y) * np.ones([y.shape[0], 1])\n",
    "\n",
    "        if loss_name == \"mse\":\n",
    "            self.objective = self.mse\n",
    "            self.objective_grad = self.mse_grad\n",
    "\n",
    "        elif loss_name == \"rmsle\":\n",
    "            self.objective = self.rmsle\n",
    "            self.objective_grad = self.rmsle_grad\n",
    "\n",
    "        self.trees_ = []\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y, p):\n",
    "        return np.mean((y - p) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_grad(y: np.array, p: np.array):\n",
    "        return 2 * (p - y) / y.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def rmsle(y, p):\n",
    "        y = y.reshape([y.shape[0], 1])\n",
    "        p = p.reshape([p.shape[0], 1])\n",
    "        return np.mean(np.log((p + 1) / (y + 1)) ** 2) ** 0.5\n",
    "\n",
    "    def rmsle_grad(self, y, p):\n",
    "        y = y.reshape([y.shape[0], 1])\n",
    "        p = p.reshape([p.shape[0], 1])\n",
    "        return 1.0 / (y.shape[0] * (p + 1) * self.rmsle(y, p)) * np.log((p + 1) / (y + 1))\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        b = self.initialization(y)\n",
    "        prediction = b.copy()\n",
    "\n",
    "        for t in tqdm(range(self.n_estimators)):\n",
    "            if t == 0:\n",
    "                resid = y\n",
    "            else:\n",
    "                resid = -self.objective_grad(y, prediction)\n",
    "\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                criterion=self.criterion,\n",
    "                leaf_func=self.leaf_func\n",
    "            )\n",
    "            tree.fit(X, pd.Series(resid))\n",
    "            b = tree.predict(X).reshape([X.shape[0], 1])\n",
    "            self.trees_.append(tree)\n",
    "            prediction += self.learning_rate * b\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.ones([X.shape[0], 1])\n",
    "        for t in range(self.n_estimators):\n",
    "            predictions += self.learning_rate * self.trees_[t].predict(X).reshape([X.shape[0], 1])\n",
    "        return predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekX75eSKOXgx"
   },
   "source": [
    "# Функции для отрисовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zDeexQEiOavB"
   },
   "source": [
    "def show_histplot(data: pd.DataFrame, feature_name: str):\n",
    "    sns.histplot(data, kde=True, binwidth=0.1)\n",
    "    plt.xlabel(f'Значения {feature_name}')\n",
    "    plt.ylabel('Частота')\n",
    "    plt.title(f'Распределение {feature_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_boxplot(df_column, column_name):\n",
    "    pd.DataFrame(df_column).boxplot(sym='o', whis=1.0, showmeans=True)\n",
    "    plt.show()\n",
    "\n",
    "def get_heatmap(df: pd.DataFrame):\n",
    "    sns.heatmap(df)\n",
    "\n",
    "def get_3d(param1: list[int], param2: list[int], result: list[int], name_param1: str, name_param2: str):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.plot3D(param1, param2, result, 'green')\n",
    "    ax.set_title(f'Зависимость метрики R² от {name_param1} и {name_param2}')\n",
    "    plt.show()\n",
    "\n",
    "def get_2d(param1: list[int], result: list[int], name_param1: str):\n",
    "    plt.title(f'Зависимость метрики R² от {name_param1}')\n",
    "    plt.plot(param1, result)\n",
    "\n",
    "\n",
    "def output_roc_auc(y_test: pd.Series, preds: pd.Series):\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.set_color_codes(\"muted\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, preds, pos_label=1)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, lw=lw, label='ROC curve ')\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.savefig(\"ROC.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "def get_bias_variance(model_param, bias_model, var_model,\n",
    "                      avg_model, name_param, name_model):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.xlabel(f'Different Values of {name_param}')\n",
    "    plt.ylabel(f'Tradeoff bias-variance {name_model}')\n",
    "    plt.plot(model_param, bias_model, color = 'r', label = \"avg_bias\")\n",
    "    plt.plot(model_param, var_model, color = 'b', label = 'avg_var')\n",
    "    plt.plot(model_param, avg_model, color = 'g', label = 'avg_tree')\n",
    "    plt.legend(bbox_to_anchor=(1, 1),bbox_transform=plt.gcf().transFigure)\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Функции для обучения/теста моделей"
   ],
   "metadata": {
    "id": "2_hWHpz5-Tfj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_tree(param, X_train, y_train, X_test, y_test, name_param):\n",
    "    new_model = DecisionTree()\n",
    "    new_model.name_param = int(param)\n",
    "    new_model.fit(X_train, y_train)\n",
    "\n",
    "    preds_model = new_model.predict(X_test)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_model),\n",
    "                                             label_encoder.inverse_transform(y_test))\n",
    "\n",
    "    variance = np.var(preds_model)\n",
    "    sse = np.mean((np.mean(preds_model) - y_test) ** 2)\n",
    "    bias = sse - variance\n",
    "\n",
    "    return mean_f1['macro avg']['f1-score'], bias, variance, sse\n",
    "\n",
    "\n",
    "def trainer_one_param(func_train, X_train, y_train, X_test, y_test, arr_param, name_param, *args):\n",
    "    results = []\n",
    "    arr_var = []\n",
    "    arr_sse = []\n",
    "    arr_bias = []\n",
    "\n",
    "    with Pool() as pool:\n",
    "        for result in tqdm(pool.starmap(func_train, [(param, X_train, y_train, X_test, y_test, name_param) for param in arr_param]),\n",
    "                           total=len(arr_param), desc=\"перебираем параметры\"):\n",
    "            f1_score, bias, variance, sse = result\n",
    "            results.append(f1_score)\n",
    "            arr_bias.append(bias)\n",
    "            arr_var.append(variance)\n",
    "            arr_sse.append(sse)\n",
    "\n",
    "    return results, arr_bias, arr_var, arr_sse"
   ],
   "metadata": {
    "id": "-Kyjgfki-XYd"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvLSWDxOOez8"
   },
   "source": [
    "# Всякие полезные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n7VJx_PkOivx"
   },
   "source": [
    "# Evaluation function\n",
    "def evaluation(model_name, y_test, y_pred_test, output=False):\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    if output:\n",
    "        print(f'\\n{model_name} Evaluation:')\n",
    "        print(f'Test R²: {r2_test:.7f}')\n",
    "        print(f'Test RMSE: {rmse_test:.5f}')\n",
    "    return r2_test\n",
    "\n",
    "def output_metrics_classification(y_test: pd.Series, preds: pd.Series):\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "    return report\n",
    "\n",
    "def show_dependencies(num_layers, train_results, test_results):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(num_layers, test_results, marker='o', label='Test Results', color='b')\n",
    "    plt.plot(num_layers, train_results, marker='o', label='Train Results', color='g')\n",
    "    plt.title('Зависимость тестовых и тренировочных результатов от количества слоев')\n",
    "    plt.xlabel('Количество слоев (num_layers)')\n",
    "    plt.ylabel('Результаты')\n",
    "    plt.xticks(num_layers)\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXLW3fiNVO8K"
   },
   "source": [
    "# Импорт датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83kIz-s8eLad"
   },
   "source": [
    "Пол/Возраст/Рост/Вес - это физические характеристики\n",
    "\n",
    "family_history_with_overweight -  семейная история с избыточным весом\n",
    "\n",
    "FAVC - Частое употребление высококалорийной пищи\n",
    "\n",
    "FCVC - Частота употребления овощей\n",
    "\n",
    "NCP - Количество основных приемов пищи\n",
    "\n",
    "CAEC - Количество приемов пищи между приемами пищи\n",
    "\n",
    "SMOKE - употребление табака\n",
    "\n",
    "CH20 - Ежедневное потребление воды\n",
    "\n",
    "SCC - Контроль потребления калорий\n",
    "\n",
    "FAF - Частота физической активности\n",
    "\n",
    "ВТ - Время, в течение которого вы пользуетесь техническими устройствами\n",
    "\n",
    "CALC - Потребление алкоголя\n",
    "\n",
    "MTRANS - Использованный транспорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Jg-DcBduVQMq",
    "outputId": "0fa8552e-2cab-46fc-dec6-6c73157169b7"
   },
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/Всякие датасеты/train_fat.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvQ1V7-ZUuC-"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fl06gdH6UxSA",
    "outputId": "84436ab5-6313-4bab-c9e0-ab840c51e0b5"
   },
   "source": [
    "categorical_features = df.columns[df.dtypes==\"object\"].tolist()\n",
    "numeric_features = df.columns[df.dtypes!=\"object\"].tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    df[feature] = label_encoder.fit_transform((df.loc[:, feature]))\n",
    "\n",
    "categorical_features = df.columns[df.dtypes==\"bool\"].tolist()\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ltwWJlML7Gd"
   },
   "source": [
    "# EDA: Obesity Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-lK3ikyPGQc"
   },
   "source": [
    "## Чекаем дупликаты и Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owHqlOM1OQ_4",
    "outputId": "67bfe2a4-f30a-4bd3-8d45-8151c162d356"
   },
   "source": [
    "print(f\"Number of missing value:{df.isna().sum().sum()}\")\n",
    "print(f\"Number of duplicated value:{df.duplicated().sum()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "319ElWXGaVCH"
   },
   "source": [
    "df = df.dropna(axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTk5xL3EPs2C"
   },
   "source": [
    "## Распределение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YfdYe7_rPu6S",
    "outputId": "28aeadd6-97da-433f-ac81-a94a4d40cb8a"
   },
   "source": [
    "[show_histplot(df[column], column) for column in df.columns]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0y_dJKijjOB"
   },
   "source": [
    "**Итог**\n",
    "\n",
    "Таргет имеет гипергеометрическое распределение, то есть модели будет относительно сложно его повторить. Спасибо, что не рандомное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUHdN8iFPvL-"
   },
   "source": [
    "## Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZUmf43De6S8"
   },
   "source": [
    "### Ящики с усами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0z5wMaOZPyA_",
    "outputId": "3519cf85-e737-4895-fc14-9dc96f6a6810"
   },
   "source": [
    "[get_boxplot(df[column], column) for column in df.columns]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciVstC6Rew4m"
   },
   "source": [
    "### Смотрим на выбросы в процентах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLb1AW3ve0OL",
    "outputId": "4d5461dc-a110-4e28-9d03-aee207a29921"
   },
   "source": [
    "def find_outliers(df):\n",
    "    outliers = {}\n",
    "    for col in df.columns:\n",
    "        v = df[col]\n",
    "        q1 = v.quantile(0.25)\n",
    "        q3 = v.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers_count = ((v < lower_bound) | (v > upper_bound)).sum()\n",
    "        perc = outliers_count * 100.0 / len(df)\n",
    "        outliers[col] = (perc, outliers_count)\n",
    "        print(f\"Column {col} outliers = {perc:.2f}%\")\n",
    "\n",
    "    return outliers\n",
    "\n",
    "outliers = find_outliers(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHmnRB0RihCH"
   },
   "source": [
    "**Итог**\n",
    "\n",
    "Можно заметить, что некоторые признаки имеют большой процент выбросов, но на самом деле это не выбросы, а статистика и ее нужно учитывать, поэтому я не буду ничего с ними делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbus5WQndHek"
   },
   "source": [
    " ## Тепловая карта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qBGdh8v1dKHC",
    "outputId": "d53ea3bb-2d1b-4fca-dd52-c01049163958"
   },
   "source": [
    "sns.heatmap(df.corr(method='spearman'), vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V89MZnmpedxN"
   },
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-UBDqdRcfJVv"
   },
   "source": [
    "X = df\n",
    "y = X['NObeyesdad']\n",
    "X = X.drop('NObeyesdad', axis=1, inplace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "J2vWiivqeiIE",
    "outputId": "81cad0b0-5b55-4a82-8d9c-9693471add10"
   },
   "source": [
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "discrete_features = X.dtypes == int\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJgcbwUXfXX4"
   },
   "source": [
    "**Итог**\n",
    "\n",
    "Предлагаю удалить SMOKE, FAVC, MTRANS, CALC, потому что они не дают никакой связи ни на тепловой карте Спирмена, ни на графике взаимной информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KD3PaA7MfmyU"
   },
   "source": [
    "X = X.drop(['SMOKE', 'FAVC', 'MTRANS', 'CALC'], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVEvJt0SPyQZ"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4XeF4Vvdmwvu"
   },
   "source": [
    "X['mean'] = X[X.columns].mean(axis=1)\n",
    "X['std'] = X[X.columns].std(axis=1)\n",
    "X['max'] = X[X.columns].max(axis=1)\n",
    "X['median'] = X[X.columns].median(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtEvI53BP86H"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Y5PiCv9pP_th",
    "outputId": "aae9205b-6d93-41c2-f876-28472d512410"
   },
   "source": [
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "new_pca_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=component_names,\n",
    "    index=X.columns,\n",
    ")\n",
    "new_pca_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5icInYZ-ZVWn",
    "outputId": "d541ef77-4797-4761-ea0c-49d339ba5e72"
   },
   "source": [
    "plot_variance(pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jUmDJ7skei2"
   },
   "source": [
    "**Замечание**\n",
    "\n",
    "Давайте оставим только 10 компонент, так как они объясняют процентов 90% всей информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XPvyaapBkwrf",
    "outputId": "c6de1fe9-d4c5-4278-eca4-5562b2aad67d"
   },
   "source": [
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=component_names,\n",
    "    index=X.columns,\n",
    ")\n",
    "X_pca"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMqZUEF-oMw6"
   },
   "source": [
    "# Разделение на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KI-4En71oTyU"
   },
   "source": [
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eJG6TUDaomJ2"
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKvMiYdhuvD0"
   },
   "source": [
    "# Поговорим про метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZklpGNRL1LIk"
   },
   "source": [
    "***На какую метрику мне больше всего выгоднее смотреть?***\n",
    "\n",
    "- Precision показывает долю правильно-положительных ответов среди всех положительных ответов.  То есть Precision демонстрирует способность модели отличать этот класс от других.\n",
    "\n",
    "- Recall показывает долю правильно-положительных ответов среди всех положительных. То есть Recall демонстрирует способность модели находить этот класс в целом.\n",
    "\n",
    "- F-1 - баланс между этими двумя метриками, очев.\n",
    "\n",
    "\n",
    "По-хорошему надо бы смотреть на все три метрики, но на F-1 и Recall в особенности. Потому что F-1 более информативная, а Recall нам нужно здесь больше, так как если модель пропустит человека с высокой степень ожирения и его не проверят, то будет намного хуже (Recall) чем если бы модель причислила класс высокого ожирения человеку, у которого его нет (Precision).\n",
    "\n",
    "Лучше проверить, чем недопроверить!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn40kL2ynBI1"
   },
   "source": [
    "# Обучение/тест - дерево решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0PZimdZ9s_d"
   },
   "source": [
    "## Моя реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaFCG6FL9wAt"
   },
   "source": [
    "### Подбираем параметр min_samples_split, не ограничевая высоту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IJy_7trjnE49"
   },
   "source": [
    "arr_split = range(10, 30, 3)\n",
    "results, arr_var, arr_sse, arr_bias = [], [], [], []\n",
    "for split in arr_split:\n",
    "    tree = DecisionTree(max_depth=100, min_samples_split=split, criterion=\"entropy\", leaf_func=\"classification_leaf\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])\n",
    "\n",
    "    variance = np.var(preds_tree)\n",
    "    sse = np.mean((np.mean(preds_tree) - y)** 2)\n",
    "    bias = sse - variance\n",
    "\n",
    "    arr_var.append(variance)\n",
    "    arr_sse.append(sse)\n",
    "    arr_bias.append(bias)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "tlEgXnqOU0vC",
    "outputId": "511534cb-17a9-43a0-d271-d8f03ec7ee5f"
   },
   "source": [
    "get_2d(arr_split, results, 'min_samples_split')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_bias_variance(arr_split, arr_bias, arr_var, arr_sse, \"min_samples_leaf\", \"Decision Tree\")"
   ],
   "metadata": {
    "id": "HlgWvcRKFHdH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCd-19IfeIq_"
   },
   "source": [
    "### Подбираем параметр min_samples_leaf, не ограничивая высоту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LLZatBw5SjAV"
   },
   "source": [
    "arr_leaf = range(7, 25, 3)\n",
    "results, arr_var, arr_sse, arr_bias = [], [], [], []\n",
    "for leaf in arr_leaf:\n",
    "    tree = DecisionTree(max_depth=100, min_samples_leaf=leaf, criterion=\"entropy\", leaf_func=\"classification_leaf\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])\n",
    "\n",
    "    variance = np.var(preds_tree)\n",
    "    sse = np.mean((np.mean(preds_tree) - y)** 2)\n",
    "    bias = sse - variance\n",
    "\n",
    "    arr_var.append(variance)\n",
    "    arr_sse.append(sse)\n",
    "    arr_bias.append(bias)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "PP1EWFwsU0Hz",
    "outputId": "4ba9e624-4d53-492c-a367-fbd908d70ce3"
   },
   "source": [
    "get_2d(arr_leaf, results, 'min_samples_leaf')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_bias_variance(arr_leaf, arr_bias, arr_var, arr_sse, \"min_samples_leaf\", \"Decision Tree\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "gA3kA8p-Pbie",
    "outputId": "e0aaee74-bea9-459d-f42f-866d7b14ffa0"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnBbkEIYRn_0"
   },
   "source": [
    "### Переберем высоты и посмотрим, какая лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oy3Xh8lQRte3",
    "outputId": "029b61e5-5322-467b-ebeb-c4574a5fcddf"
   },
   "source": [
    "arr_depth = range(10, 30, 2)\n",
    "results = []\n",
    "for depth in tqdm(arr_depth):\n",
    "    tree = DecisionTree(max_depth=depth, min_samples_split=10, min_samples_leaf=7, criterion=\"entropy\", leaf_func=\"classification_leaf\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_2d(arr_depth, results, 'min_samples_leaf')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "XCS1WzxZUL_C",
    "outputId": "524dac1a-cd0e-4669-926b-812ad830b9f2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC5dhnv_wBDK"
   },
   "source": [
    "**Итог:**\n",
    "Давайте посмотрим на метрики последних двух классов: Overweight_Level_II и Overweight_Level_I.\n",
    "- маленькая Precision у того и того класса означает, что моя модель дала дала много ложно-положительных ответов. То есть она причисляет объектам с отрицательным таргетом положительный класс.\n",
    "\n",
    "- маленькая Recall у того и того класса означает, что моя модель дала дала много ложно-отрицательных ответов. То есть она причисляет объект с положительным таргетом к другим классам.\n",
    "\n",
    "**Вывод такой:** модель не умеет не отличать эти класс от других, не находить их.\n",
    "\n",
    " Почему так произошло? Потому что судя по support и распределению таргета, имеется дисбаланс классов, а дерево решений зависимо от баланса класов.\n",
    "\n",
    "Скорее всего последние два класса путаются с Obesity_Type_I и Obesity_Type_III"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sklearn реализация"
   ],
   "metadata": {
    "id": "uM-zpSQpT8QX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подбираем параметр min_samples_split, не ограничевая высоту"
   ],
   "metadata": {
    "id": "_eTQ5xCfUvnb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "arr_split = range(10, 30, 3)\n",
    "best_f1 = -1\n",
    "results = []\n",
    "for split in arr_split:\n",
    "    tree = DecisionTreeClassifier(max_depth=100, min_samples_split=split, criterion=\"entropy\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])"
   ],
   "metadata": {
    "id": "NOuhywWkT_1T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_2d(arr_split, results, 'min_samples_split')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "e48rKn3KU73h",
    "outputId": "f4975f8f-30ed-412f-bd2e-7be40c008881"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подбираем параметр min_samples_leaf, не ограничивая высоту"
   ],
   "metadata": {
    "id": "KvAr9AtZU8mZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "arr_leaf = range(7, 25, 3)\n",
    "best_f1 = -1\n",
    "results = []\n",
    "for leaf in arr_leaf:\n",
    "    tree = DecisionTreeClassifier(max_depth=100, min_samples_leaf=leaf, criterion=\"entropy\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])"
   ],
   "metadata": {
    "id": "xiKte17PU8y6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_2d(arr_leaf, results, 'min_samples_leaf')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Y4CC_GYYVJSb",
    "outputId": "0150aba7-046b-4271-f71a-4eed7d5fb34e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Переберем высоты и посмотрим, какая лучше"
   ],
   "metadata": {
    "id": "FQ1a2Ik0VaQ6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "arr_depth = range(10, 30, 2)\n",
    "results = []\n",
    "for depth in tqdm(arr_depth):\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, min_samples_split=25, min_samples_leaf=11, criterion=\"entropy\")\n",
    "    tree.fit(X_train_pca, y_train_pca)\n",
    "    preds_tree = tree.predict(X_test_pca)\n",
    "    mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "    results.append(mean_f1['macro avg']['f1-score'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvWVqmHgVayd",
    "outputId": "dec7836d-d306-4dc7-b8ef-33f4056dd282"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_2d(arr_depth, results, 'max_depth')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "GJnl5w9EVe0i",
    "outputId": "5b546283-e234-43ce-fd5a-9e05066b940c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Случайный лес"
   ],
   "metadata": {
    "id": "1hggLDQfV40q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Моя реализация"
   ],
   "metadata": {
    "id": "ehlPH1sQWGKS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подбор гиперпараметров"
   ],
   "metadata": {
    "id": "Mr-gdP9uWV55"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "arr_split = range(10, 30, 3)\n",
    "max_depths = range(7, 20, 2)\n",
    "n_estimators = range(5, 15, 2)\n",
    "best_f1 = -1\n",
    "results = []\n",
    "for split in tqdm(arr_split):\n",
    "    for depth in max_depths:\n",
    "      for estimators in n_estimators:\n",
    "        tree = RandomForest(max_depth=depth, min_samples_split=split, min_samples_leaf=8, criterion=\"entropy\", leaf_func=\"classification_leaf\", N=estimators)\n",
    "        tree.fit(X_train_pca, y_train_pca)\n",
    "        preds_tree = tree.predict(X_test_pca)\n",
    "        mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "        if best_f1 < mean_f1['macro avg']['f1-score']:\n",
    "          best_f1 = mean_f1\n",
    "          best_depth = depth\n",
    "          best_split = split\n",
    "          best_n_estimators = estimators\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "collapsed": true,
    "id": "rII1ec9eV7OB",
    "outputId": "b24e0d79-85b5-431c-da8e-43fcd61ab528"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TfXfBGM4ZTgt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Построим зависимость от количества деревьев"
   ],
   "metadata": {
    "id": "VbZ2U6PPYm7w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_estimators = range(5, 15, 2)\n",
    "results = []\n",
    "for estimators in tqdm(n_estimators):\n",
    "        tree = RandomForest(max_depth=15, min_samples_split=10, min_samples_leaf=8, criterion=\"entropy\", leaf_func=\"classification_leaf\", N=estimators)\n",
    "        tree.fit(X_train_pca, y_train_pca)\n",
    "        preds_tree = tree.predict(X_test_pca)\n",
    "        mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "        results.append(mean_f1['macro avg']['f1-score'])"
   ],
   "metadata": {
    "collapsed": true,
    "id": "jOQJEScWY3dH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_2d(arr_split, results, 'min_samples_split')"
   ],
   "metadata": {
    "id": "1O-27rgCZjvO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sklearn реализация"
   ],
   "metadata": {
    "id": "oMk0g45Ic6G5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подбор гиперпараметров"
   ],
   "metadata": {
    "id": "Hzv4z3tVdB4P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "arr_split = range(10, 30, 3)\n",
    "max_depths = range(7, 20, 2)\n",
    "n_estimators = range(5, 15, 2)\n",
    "best_f1 = -1\n",
    "results = []\n",
    "for split in tqdm(arr_split):\n",
    "    for depth in max_depths:\n",
    "      for estimators in n_estimators:\n",
    "        tree = RandomForestClassifier(max_depth=depth, min_samples_split=split,criterion=\"entropy\", n_estimators=estimators)\n",
    "        tree.fit(X_train_pca, y_train_pca)\n",
    "        preds_tree = tree.predict(X_test_pca)\n",
    "        mean_f1 = output_metrics_classification(label_encoder.inverse_transform(preds_tree), label_encoder.inverse_transform(y_test_pca))\n",
    "        if best_f1 < mean_f1['macro avg']['f1-score']:\n",
    "          best_f1 = mean_f1['macro avg']['f1-score']\n",
    "          best_depth = depth\n",
    "          best_split = split\n",
    "          best_n_estimators = estimators\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJDfxqbzc89o",
    "outputId": "750b229f-5058-476c-f4ec-77557f7fcc5c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"best_f1 = {best_f1}, best_depth = {best_depth}, best_split = {best_split}, best_n_estimators = {best_n_estimators}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJDWD0P1fn-j",
    "outputId": "a3eabd12-3874-4ba9-dc81-f9df1b244897"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Смотрим на зависимости от количества деревьев на train/test датасете"
   ],
   "metadata": {
    "id": "WxJzHda-fyf-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results_train = []\n",
    "results_test = []\n",
    "n_estimators = range(5, 20, 2)\n",
    "for estimators in n_estimators:\n",
    "      tree = RandomForestClassifier(max_depth=depth, min_samples_split=split,criterion=\"entropy\", n_estimators=estimators)\n",
    "      tree.fit(X_train_pca, y_train_pca)\n",
    "      preds_tree_test = tree.predict(X_test_pca)\n",
    "      mean_f1_test = output_metrics_classification(label_encoder.inverse_transform(preds_tree_test), label_encoder.inverse_transform(y_test_pca))\n",
    "      results_test.append(mean_f1_test['macro avg']['f1-score'])\n",
    "\n",
    "      preds_tree_train = tree.predict(X_train_pca)\n",
    "      mean_f1_train = output_metrics_classification(label_encoder.inverse_transform(preds_tree_train), label_encoder.inverse_transform(y_train_pca))\n",
    "      results_train.append(mean_f1_train['macro avg']['f1-score'])"
   ],
   "metadata": {
    "id": "ql_J0wjDgBZC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_dependencies(n_estimators, results_train, results_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "CMvghSFshNIG",
    "outputId": "663d0bd2-6a2a-4659-9b67-320fc720fe31"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Градиентный бустинг"
   ],
   "metadata": {
    "id": "vFJqL6RicJcj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Моя реализация"
   ],
   "metadata": {
    "id": "LMuUdefelBAV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results_train = []\n",
    "results_test = []\n",
    "n_estimators = range(5, 20, 2)\n",
    "for estimators in tqdm(n_estimators):\n",
    "      tree = GradientBoostingClassifier(n_estimators=10, learning_rate=0.01, max_depth=3, min_samples_split=5, criterion=\"variance\",\n",
    "                 leaf_func=\"regression_leaf\", random_state=17, loss_name=\"mse\")\n",
    "      tree.fit(X_train_pca, y_train_pca)\n",
    "      preds_tree_test = tree.predict(X_test_pca)\n",
    "      mean_f1_test = output_metrics_classification(label_encoder.inverse_transform(preds_tree_test), label_encoder.inverse_transform(y_test_pca))\n",
    "      results_test.append(mean_f1_test['macro avg']['f1-score'])\n",
    "\n",
    "      preds_tree_train = tree.predict(X_train_pca)\n",
    "      mean_f1_train = output_metrics_classification(label_encoder.inverse_transform(preds_tree_train), label_encoder.inverse_transform(y_train_pca))\n",
    "      results_train.append(mean_f1_train['macro avg']['f1-score'])"
   ],
   "metadata": {
    "id": "CpkouFTjlDbx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_dependencies(n_estimators, results_train, results_test)"
   ],
   "metadata": {
    "id": "5OosRHxnlHe8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sklearn реализация"
   ],
   "metadata": {
    "id": "YDteYPalcNP0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results_train = []\n",
    "results_test = []\n",
    "n_estimators = range(5, 20, 2)\n",
    "for estimators in tqdm(n_estimators):\n",
    "      tree = GradientBoostingClassifier(criterion=\"squared_error\", n_estimators=estimators)\n",
    "      tree.fit(X_train_pca, y_train_pca)\n",
    "      preds_tree_test = tree.predict(X_test_pca)\n",
    "      mean_f1_test = output_metrics_classification(label_encoder.inverse_transform(preds_tree_test), label_encoder.inverse_transform(y_test_pca))\n",
    "      results_test.append(mean_f1_test['macro avg']['f1-score'])\n",
    "\n",
    "      preds_tree_train = tree.predict(X_train_pca)\n",
    "      mean_f1_train = output_metrics_classification(label_encoder.inverse_transform(preds_tree_train), label_encoder.inverse_transform(y_train_pca))\n",
    "      results_train.append(mean_f1_train['macro avg']['f1-score'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lutKhGWacL2h",
    "outputId": "0548249a-e568-4490-ac01-9c29e031230d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_dependencies(n_estimators, results_train, results_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "T095DJG9kEVr",
    "outputId": "e1febdb4-e003-43be-ea7f-16b960811a8c"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gU1TknRxLmIf",
    "9HYHYnZHLZtv",
    "u1QrTds1LdjQ",
    "axrszEPELhj1",
    "ekX75eSKOXgx",
    "2_hWHpz5-Tfj",
    "kvLSWDxOOez8",
    "vXLW3fiNVO8K",
    "AvQ1V7-ZUuC-",
    "7ltwWJlML7Gd",
    "M-lK3ikyPGQc",
    "hTk5xL3EPs2C",
    "gUHdN8iFPvL-",
    "YZUmf43De6S8",
    "ciVstC6Rew4m",
    "Tbus5WQndHek",
    "V89MZnmpedxN",
    "KVEvJt0SPyQZ",
    "GtEvI53BP86H",
    "GMqZUEF-oMw6",
    "FKvMiYdhuvD0",
    "jCd-19IfeIq_",
    "KnBbkEIYRn_0",
    "uM-zpSQpT8QX",
    "_eTQ5xCfUvnb",
    "KvAr9AtZU8mZ"
   ],
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
